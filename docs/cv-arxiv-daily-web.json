{"MuZero": {"2504.14636": "|**2025-04-20**|**AlphaZero-Edu: Making AlphaZero Accessible to Everyone**|Binjie Guo et.al.|[2504.14636](http://arxiv.org/abs/2504.14636)|**[link](https://github.com/starlight1212/alphazero_edu)**|\n", "2504.07757": "|**2025-04-10**|**Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency**|Ameya Joshi et.al.|[2504.07757](http://arxiv.org/abs/2504.07757)|null|\n", "2504.07091": "|**2025-04-09**|**AssistanceZero: Scalably Solving Assistance Games**|Cassidy Laidlaw et.al.|[2504.07091](http://arxiv.org/abs/2504.07091)|**[link](https://github.com/cassidylaidlaw/minecraft-building-assistance-game)**|\n", "2504.05425": "|**2025-04-07**|**A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%**|Benny Skidanov et.al.|[2504.05425](http://arxiv.org/abs/2504.05425)|null|\n", "2503.21047": "|**2025-03-26**|**World Model Agents with Change-Based Intrinsic Motivation**|Jeremias Ferrao et.al.|[2503.21047](http://arxiv.org/abs/2503.21047)|**[link](https://github.com/Jazhyc/world-model-policy-transfer)**|\n", "2503.13178": "|**2025-03-17**|**Rapfi: Distilling Efficient Neural Network for the Game of Gomoku**|Zhanggen Jin et.al.|[2503.13178](http://arxiv.org/abs/2503.13178)|null|\n", "2503.10822": "|**2025-05-17**|**Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science**|Johannes Buchner et.al.|[2503.10822](http://arxiv.org/abs/2503.10822)|null|\n", "2503.08872": "|**2025-03-11**|**Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing**|Cameron Redovian et.al.|[2503.08872](http://arxiv.org/abs/2503.08872)|null|\n", "2503.05573": "|**2025-03-07**|**InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model**|Feeza Khan Khanzada et.al.|[2503.05573](http://arxiv.org/abs/2503.05573)|null|\n", "2503.04416": "|**2025-05-25**|**Learning Transformer-based World Models with Contrastive Predictive Coding**|Maxime Burchi et.al.|[2503.04416](http://arxiv.org/abs/2503.04416)|null|\n", "2503.02279": "|**2025-03-04**|**DreamerV3 for Traffic Signal Control: Hyperparameter Tuning and Performance**|Qiang Li et.al.|[2503.02279](http://arxiv.org/abs/2503.02279)|null|\n", "2503.00653": "|**2025-03-01**|**Discrete Codebook World Models for Continuous Control**|Aidan Scannell et.al.|[2503.00653](http://arxiv.org/abs/2503.00653)|**[link](https://github.com/aidanscannell/dcmpc)**|\n", "2502.16634": "|**2025-03-21**|**OptionZero: Planning with Learned Options**|Po-Wei Huang et.al.|[2502.16634](http://arxiv.org/abs/2502.16634)|**[link](https://github.com/rlglab/optionzero)**|\n", "2503.05748": "|**2025-02-20**|**Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective**|Krti Tallam et.al.|[2503.05748](http://arxiv.org/abs/2503.05748)|null|\n", "2502.13918": "|**2025-02-19**|**Playing Hex and Counter Wargames using Reinforcement Learning and Recurrent Neural Networks**|Guilherme Palma et.al.|[2502.13918](http://arxiv.org/abs/2502.13918)|**[link](https://github.com/guilherme439/nuzero)**|\n", "2502.15777": "|**2025-02-17**|**TSS GAZ PTP: Towards Improving Gumbel AlphaZero with Two-stage Self-play for Multi-constrained Electric Vehicle Routing Problems**|Hui Wang et.al.|[2502.15777](http://arxiv.org/abs/2502.15777)|null|\n", "2502.10303": "|**2025-02-14**|**Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations**|Abdelrhman Shaheen et.al.|[2502.10303](http://arxiv.org/abs/2502.10303)|null|\n", "2502.06773": "|**2025-02-10**|**On the Emergence of Thinking in LLMs I: Searching for the Right Intuition**|Guanghao Ye et.al.|[2502.06773](http://arxiv.org/abs/2502.06773)|**[link](https://github.com/GuanghaoYe/Emergence-of-Thinking)**|\n", "2502.05555": "|**2025-02-08**|**Efficient Reinforcement Learning Through Adaptively Pretrained Visual Encoder**|Yuhan Zhang et.al.|[2502.05555](http://arxiv.org/abs/2502.05555)|null|\n", "2502.01591": "|**2025-02-03**|**Improving Transformer World Models for Data-Efficient RL**|Antoine Dedieu et.al.|[2502.01591](http://arxiv.org/abs/2502.01591)|null|\n", "2501.14377": "|**2025-01-24**|**Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone Flight**|Angel Romero et.al.|[2501.14377](http://arxiv.org/abs/2501.14377)|null|\n", "2412.17397": "|**2024-12-23**|**Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning**|Huchen Jiang et.al.|[2412.17397](http://arxiv.org/abs/2412.17397)|null|\n", "2412.15904": "|**2025-03-08**|**What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning**|Yiran Ma et.al.|[2412.15904](http://arxiv.org/abs/2412.15904)|null|\n", "2412.11979": "|**2024-12-16**|**AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws**|Oren Neumann et.al.|[2412.11979](http://arxiv.org/abs/2412.11979)|**[link](https://github.com/orenneumann/alphazero_zipfs_law)**|\n", "2412.05766": "|**2024-12-08**|**Policy-shaped prediction: avoiding distractions in model-based reinforcement learning**|Miles Hutson et.al.|[2412.05766](http://arxiv.org/abs/2412.05766)|null|\n", "2411.08794": "|**2024-11-13**|**Evaluating World Models with LLM for Decision Making**|Chang Yang et.al.|[2411.08794](http://arxiv.org/abs/2411.08794)|null|\n", "2411.06403": "|**2024-11-10**|**Mastering NIM and Impartial Games with Weak Neural Networks: An AlphaZero-inspired Multi-Frame Approach**|S\u00f8ren Riis et.al.|[2411.06403](http://arxiv.org/abs/2411.06403)|null|\n", "2411.04915": "|**2024-11-07**|**Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping**|Bavo Lesy et.al.|[2411.04915](http://arxiv.org/abs/2411.04915)|null|\n", "2411.04580": "|**2024-11-07**|**Interpreting the Learned Model in MuZero Planning**|Hung Guei et.al.|[2411.04580](http://arxiv.org/abs/2411.04580)|null|\n", "2410.23753": "|**2024-10-31**|**Enhancing Chess Reinforcement Learning with Graph Representation**|Tomas Rigaux et.al.|[2410.23753](http://arxiv.org/abs/2410.23753)|**[link](https://github.com/akulen/alphagateau)**|\n", "2410.14616": "|**2024-10-18**|**Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments**|Mariusz Wisniewski et.al.|[2410.14616](http://arxiv.org/abs/2410.14616)|**[link](https://github.com/mazqtpopx/cranfield-navigation-gym)**|\n", "2410.11234": "|**2025-05-19**|**Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning**|Jiayu Chen et.al.|[2410.11234](http://arxiv.org/abs/2410.11234)|**[link](https://github.com/lucascjysdl/offline-rl-kit)**|\n", "2410.08822": "|**2025-02-07**|**SOLD: Slot Object-Centric Latent Dynamics Models for Relational Manipulation Learning from Pixels**|Malte Mosbach et.al.|[2410.08822](http://arxiv.org/abs/2410.08822)|null|\n", "2410.05347": "|**2024-10-07**|**ResTNet: Defense against Adversarial Policies via Transformer in Computer Go**|Tai-Lin Wu et.al.|[2410.05347](http://arxiv.org/abs/2410.05347)|null|\n", "2409.20553": "|**2024-10-31**|**Maia-2: A Unified Model for Human-AI Alignment in Chess**|Zhenwei Tang et.al.|[2409.20553](http://arxiv.org/abs/2409.20553)|**[link](https://github.com/csslab/maia2)**|\n", "2409.20326": "|**2025-03-18**|**MARLadona -- Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning**|Zichong Li et.al.|[2409.20326](http://arxiv.org/abs/2409.20326)|null|\n", "2409.12272": "|**2024-10-28**|**Mastering Chess with a Transformer Model**|Daniel Monroe et.al.|[2409.12272](http://arxiv.org/abs/2409.12272)|**[link](https://github.com/ergodice/lczero-training)**|\n", "2408.14855": "|**2024-08-27**|**Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL**|Jihwan Lee et.al.|[2408.14855](http://arxiv.org/abs/2408.14855)|null|\n", "2408.13871": "|**2024-11-29**|**AlphaViT: A Flexible Game-Playing AI for Multiple Games and Variable Board Sizes**|Kazuhisa Fujita et.al.|[2408.13871](http://arxiv.org/abs/2408.13871)|**[link](https://github.com/kazuhisafujita/alphavit)**|\n", "2408.09858": "|**2024-10-02**|**ShortCircuit: AlphaZero-Driven Circuit Design**|Dimitrios Tsaras et.al.|[2408.09858](http://arxiv.org/abs/2408.09858)|null|\n", "2505.04127": "|**2025-05-08**|**Reinforcement Learning-Aided Design of Efficient Polarization Kernels**|Yi-Ting Hong et.al.|[2505.04127](http://arxiv.org/abs/2505.04127)|**[link](https://github.com/jaco267/alphapolar)**|\n", "2505.22772": "|**2025-05-28**|**Calibrated Value-Aware Model Learning with Stochastic Environment Models**|Claas Voelcker et.al.|[2505.22772](http://arxiv.org/abs/2505.22772)|null|\n"}, "Game": {"2504.17891": "|**2025-04-24**|**Do We Need Transformers to Play FPS Video Games?**|Karmanbir Batth et.al.|[2504.17891](http://arxiv.org/abs/2504.17891)|null|\n", "2504.14636": "|**2025-04-20**|**AlphaZero-Edu: Making AlphaZero Accessible to Everyone**|Binjie Guo et.al.|[2504.14636](http://arxiv.org/abs/2504.14636)|**[link](https://github.com/starlight1212/alphazero_edu)**|\n", "2504.13541": "|**2025-04-18**|**SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents**|Avaneesh Devkota et.al.|[2504.13541](http://arxiv.org/abs/2504.13541)|null|\n", "2504.12568": "|**2025-04-17**|**Evolutionary Policy Optimization**|Zelal Su \"Lain\" Mustafaoglu et.al.|[2504.12568](http://arxiv.org/abs/2504.12568)|null|\n", "2504.12562": "|**2025-04-17**|**ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition**|Haidar Khan et.al.|[2504.12562](http://arxiv.org/abs/2504.12562)|**[link](https://github.com/facebookresearch/zerosumeval)**|\n", "2504.12045": "|**2025-04-16**|**pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild**|Jonas Myhre Schi\u00f8tt et.al.|[2504.12045](http://arxiv.org/abs/2504.12045)|null|\n", "2504.11118": "|**2025-04-15**|**Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay**|Henrik Krauss et.al.|[2504.11118](http://arxiv.org/abs/2504.11118)|null|\n", "2504.10071": "|**2025-04-14**|**Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning**|Tien Pham et.al.|[2504.10071](http://arxiv.org/abs/2504.10071)|**[link](https://github.com/tiencapham/ife)**|\n", "2504.09192": "|**2025-05-15**|**Towards More Efficient, Robust, Instance-adaptive, and Generalizable Sequential Decision making**|Zhiyong Wang et.al.|[2504.09192](http://arxiv.org/abs/2504.09192)|null|\n", "2504.07757": "|**2025-04-10**|**Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency**|Ameya Joshi et.al.|[2504.07757](http://arxiv.org/abs/2504.07757)|null|\n", "2504.20061": "|**2025-04-19**|**Research Power Ranking: Adapting the Elo System to Quantify Scientist Evaluation**|Eldar Knar et.al.|[2504.20061](http://arxiv.org/abs/2504.20061)|null|\n", "2504.07257": "|**2025-04-09**|**Better Decisions through the Right Causal World Model**|Elisabeth Dillies et.al.|[2504.07257](http://arxiv.org/abs/2504.07257)|null|\n", "2504.08000": "|**2025-04-09**|**Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning**|Jiahua Lan et.al.|[2504.08000](http://arxiv.org/abs/2504.08000)|null|\n", "2504.06255": "|**2025-04-08**|**Diagrammatic expansion for the mutual-information rate in the realm of limited statistics**|Tobias K\u00fchn et.al.|[2504.06255](http://arxiv.org/abs/2504.06255)|null|\n", "2504.05840": "|**2025-04-08**|**Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments**|Dolton Fernandes et.al.|[2504.05840](http://arxiv.org/abs/2504.05840)|null|\n", "2504.05425": "|**2025-04-07**|**A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%**|Benny Skidanov et.al.|[2504.05425](http://arxiv.org/abs/2504.05425)|null|\n", "2504.05084": "|**2025-04-07**|**Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot Motion**|Eran Beeri Bamani et.al.|[2504.05084](http://arxiv.org/abs/2504.05084)|null|\n", "2504.04783": "|**2025-04-07**|**Playing Non-Embedded Card-Based Games with Reinforcement Learning**|Tianyang Wu et.al.|[2504.04783](http://arxiv.org/abs/2504.04783)|**[link](https://github.com/wty-yy/katacr)**|\n", "2504.03024": "|**2025-04-03**|**Deep Reinforcement Learning via Object-Centric Attention**|Jannis Bl\u00fcml et.al.|[2504.03024](http://arxiv.org/abs/2504.03024)|**[link](https://github.com/VanillaWhey/OCAtariWrappers)**|\n", "2504.00040": "|**2025-03-30**|**Quantum Methods for Managing Ambiguity in Natural Language Processing**|Jurek Eisinger et.al.|[2504.00040](http://arxiv.org/abs/2504.00040)|**[link](https://github.com/jurekjurek/managingambiguity)**|\n", "2505.00279": "|**2025-05-01**|**Policies of Multiple Skill Levels for Better Strength Estimation in Games**|Kyota Kuboki et.al.|[2505.00279](http://arxiv.org/abs/2505.00279)|null|\n", "2504.21548": "|**2025-04-30**|**Leveraging Systems and Control Theory for Social Robotics: A Model-Based Behavioral Control Approach to Human-Robot Interaction**|Maria Mor\u00e3o Patr\u00edcio et.al.|[2504.21548](http://arxiv.org/abs/2504.21548)|**[link](https://github.com/marialuis-mp/MMM-Controller-for-Social-Robot)**|\n", "2505.03251": "|**2025-05-06**|**Chess variation entropy and engine relevance for humans**|Marc Barthelemy et.al.|[2505.03251](http://arxiv.org/abs/2505.03251)|null|\n", "2503.21713": "|**2025-03-27**|**Investigating Experiential Effects in Online Chess using a Hierarchical Bayesian Analysis**|Adam Gee et.al.|[2503.21713](http://arxiv.org/abs/2503.21713)|null|\n", "2503.21683": "|**2025-03-27**|**LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning**|Hui Wang et.al.|[2503.21683](http://arxiv.org/abs/2503.21683)|null|\n", "2503.21323": "|**2025-03-27**|**DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset**|Ling Feng et.al.|[2503.21323](http://arxiv.org/abs/2503.21323)|null|\n", "2503.20986": "|**2025-04-22**|**MAD Chairs: A new tool to evaluate AI**|Chris Santos-Lang et.al.|[2503.20986](http://arxiv.org/abs/2503.20986)|null|\n", "2504.07119": "|**2025-03-26**|**UAV-Assisted MEC for Disaster Response: Stackelberg Game-Based Resource Optimization**|Yafei Guo et.al.|[2504.07119](http://arxiv.org/abs/2504.07119)|null|\n", "2503.20139": "|**2025-03-26**|**Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning**|Yongshuai Liu et.al.|[2503.20139](http://arxiv.org/abs/2503.20139)|null|\n", "2503.18700": "|**2025-03-24**|**Leaper Embeddings**|Nikolai Beluhov et.al.|[2503.18700](http://arxiv.org/abs/2503.18700)|null|\n", "2503.18612": "|**2025-03-24**|**Adventurer: Exploration with BiGAN for Deep Reinforcement Learning**|Yongshuai Liu et.al.|[2503.18612](http://arxiv.org/abs/2503.18612)|null|\n", "2503.13178": "|**2025-03-17**|**Rapfi: Distilling Efficient Neural Network for the Game of Gomoku**|Zhanggen Jin et.al.|[2503.13178](http://arxiv.org/abs/2503.13178)|null|\n", "2503.12923": "|**2025-03-17**|**Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models**|Zhiyi Huang et.al.|[2503.12923](http://arxiv.org/abs/2503.12923)|null|\n", "2503.12917": "|**2025-03-17**|**Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible**|Lin-Han Jia et.al.|[2503.12917](http://arxiv.org/abs/2503.12917)|null|\n", "2503.13557": "|**2025-03-17**|**APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games**|Yifei Chen et.al.|[2503.13557](http://arxiv.org/abs/2503.13557)|null|\n", "2503.10822": "|**2025-05-05**|**Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science**|Johannes Buchner et.al.|[2503.10822](http://arxiv.org/abs/2503.10822)|null|\n", "2503.09060": "|**2025-03-12**|**StratIncon Detector: Analyzing Strategy Inconsistencies Between Real-Time Strategy and Preferred Professional Strategy in MOBA Esports**|Ruofei Ma et.al.|[2503.09060](http://arxiv.org/abs/2503.09060)|null|\n", "2503.10673": "|**2025-04-17**|**ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition**|Hisham A. Alyahya et.al.|[2503.10673](http://arxiv.org/abs/2503.10673)|**[link](https://github.com/zerosumeval/zerosumeval)**|\n", "2503.04416": "|**2025-03-06**|**Learning Transformer-based World Models with Contrastive Predictive Coding**|Maxime Burchi et.al.|[2503.04416](http://arxiv.org/abs/2503.04416)|null|\n", "2503.07639": "|**2025-03-05**|**Mixture of Experts Made Intrinsically Interpretable**|Xingyi Yang et.al.|[2503.07639](http://arxiv.org/abs/2503.07639)|null|\n", "2505.03947": "|**2025-05-06**|**Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents**|Xiang Li et.al.|[2505.03947](http://arxiv.org/abs/2505.03947)|**[link](https://github.com/alienkevin/frogger)**|\n", "2505.09114": "|**2025-05-14**|**Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer**|Minh Hoang Nguyen et.al.|[2505.09114](http://arxiv.org/abs/2505.09114)|null|\n", "2505.08417": "|**2025-05-13**|**ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models**|Avihai Giuili et.al.|[2505.08417](http://arxiv.org/abs/2505.08417)|null|\n", "2505.07592": "|**2025-05-12**|**Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study**|Matthew Russell et.al.|[2505.07592](http://arxiv.org/abs/2505.07592)|null|\n", "2505.06652": "|**2025-05-10**|**Enfoque Odychess: Un m\u00e9todo dial\u00e9ctico, constructivista y adaptativo para la ense\u00f1anza del ajedrez con inteligencias artificiales generativas**|Ernesto Giralt Hernandez et.al.|[2505.06652](http://arxiv.org/abs/2505.06652)|null|\n", "2505.11478": "|**2025-05-16**|**Automatic Reward Shaping from Confounded Offline Data**|Mingxuan Li et.al.|[2505.11478](http://arxiv.org/abs/2505.11478)|null|\n", "2505.10819": "|**2025-05-22**|**PoE-World: Compositional World Modeling with Products of Programmatic Experts**|Wasu Top Piriyakulkij et.al.|[2505.10819](http://arxiv.org/abs/2505.10819)|**[link](https://github.com/topwasu/poe-world)**|\n", "2505.15345": "|**2025-05-23**|**Hadamax Encoding: Elevating Performance in Model-Free Atari**|Jacob E. Kooi et.al.|[2505.15345](http://arxiv.org/abs/2505.15345)|**[link](https://github.com/jacobkooi/hadamax)**|\n", "2505.15306": "|**2025-05-21**|**Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One**|Yiwen Song et.al.|[2505.15306](http://arxiv.org/abs/2505.15306)|null|\n", "2505.15293": "|**2025-05-21**|**LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models**|Qianyue Hao et.al.|[2505.15293](http://arxiv.org/abs/2505.15293)|null|\n", "2505.14943": "|**2025-05-20**|**Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities**|Ross Nordby et.al.|[2505.14943](http://arxiv.org/abs/2505.14943)|**[link](https://github.com/RossNordby/SoftPromptsForEvaluation)**|\n", "2505.21552": "|**2025-05-26**|**Understanding the learned look-ahead behavior of chess neural networks**|Diogo Cruz et.al.|[2505.21552](http://arxiv.org/abs/2505.21552)|null|\n", "2505.19423": "|**2025-05-29**|**Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network**|Bingdong Li et.al.|[2505.19423](http://arxiv.org/abs/2505.19423)|null|\n", "2505.17714": "|**2025-05-23**|**PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization**|Ben Rahman et.al.|[2505.17714](http://arxiv.org/abs/2505.17714)|null|\n"}, "Doudizhu": {"2504.15798": "|**2025-04-22**|**Quantum Entanglement Autodistillation in Baryon Pair Decays**|Hai-Long Feng et.al.|[2504.15798](http://arxiv.org/abs/2504.15798)|null|\n", "2503.13980": "|**2025-03-18**|**Empowering LLMs in Decision Games through Algorithmic Data Synthesis**|Haolin Wang et.al.|[2503.13980](http://arxiv.org/abs/2503.13980)|null|\n", "2407.10279": "|**2024-09-13**|**AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding**|Chang Lei et.al.|[2407.10279](http://arxiv.org/abs/2407.10279)|**[link](https://github.com/RuBP17/AlphaDou)**|\n", "2403.14102": "|**2024-03-21**|**DouRN: Improving DouZero by Residual Neural Networks**|Yiquan Chen et.al.|[2403.14102](http://arxiv.org/abs/2403.14102)|null|\n", "2312.02561": "|**2023-12-05**|**DanZero+: Dominating the GuanDan Game through Reinforcement Learning**|Youpeng Zhao et.al.|[2312.02561](http://arxiv.org/abs/2312.02561)|**[link](https://github.com/submit-paper/Danzero_plus)**|\n", "2210.17087": "|**2022-10-31**|**DanZero: Mastering GuanDan Game with Reinforcement Learning**|Yudong Lu et.al.|[2210.17087](http://arxiv.org/abs/2210.17087)|null|\n", "2204.02558": "|**2022-04-06**|**DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning**|Youpeng Zhao et.al.|[2204.02558](http://arxiv.org/abs/2204.02558)|**[link](https://github.com/submit-paper/doudizhu)**|\n", "2203.16406": "|**2024-02-28**|**PerfectDou: Dominating DouDizhu with Perfect Information Distillation**|Guan Yang et.al.|[2203.16406](http://arxiv.org/abs/2203.16406)|**[link](https://github.com/netease-games-ai-lab-guangzhou/perfectdou)**|\n", "2106.06135": "|**2021-06-11**|**DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning**|Daochen Zha et.al.|[2106.06135](http://arxiv.org/abs/2106.06135)|**[link](https://github.com/kwai/DouZero)**|\n", "1201.3807": "|**2012-01-18**|**Electric dipole polarizability and the neutron skin**|J. Piekarewicz et.al.|[1201.3807](http://arxiv.org/abs/1201.3807)|null|\n"}, "Reward Shaping": {"2504.16272": "|**2025-04-22**|**Learning Explainable Dense Reward Shapes via Bayesian Optimization**|Ryan Koo et.al.|[2504.16272](http://arxiv.org/abs/2504.16272)|null|\n", "2504.12609": "|**2025-04-22**|**Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration**|Tyler Ga Wei Lum et.al.|[2504.12609](http://arxiv.org/abs/2504.12609)|null|\n", "2504.12000": "|**2025-04-16**|**Control of Rayleigh-B\u00e9nard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime**|Thorben Markmann et.al.|[2504.12000](http://arxiv.org/abs/2504.12000)|**[link](https://github.com/HammerLabML/RBC-Control-SARL)**|\n", "2504.10677": "|**2025-04-14**|**Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning**|Muhammad Al-Zafar Khan et.al.|[2504.10677](http://arxiv.org/abs/2504.10677)|null|\n", "2504.09777": "|**2025-04-14**|**Reasoning without Regret**|Tarun Chitra et.al.|[2504.09777](http://arxiv.org/abs/2504.09777)|null|\n", "2503.21949": "|**2025-03-27**|**Reward Design for Reinforcement Learning Agents**|Rati Devidze et.al.|[2503.21949](http://arxiv.org/abs/2503.21949)|**[link](https://github.com/adishs/neurips2021_explicable-reward-design_code)**|\n", "2503.22723": "|**2025-03-26**|**Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping**|Mohammad Saif Nazir et.al.|[2503.22723](http://arxiv.org/abs/2503.22723)|**[link](https://github.com/RizanSM/zero_shot_llms_in_HIL_RL)**|\n", "2503.18210": "|**2025-03-23**|**ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data**|Nitish Dashora et.al.|[2503.18210](http://arxiv.org/abs/2503.18210)|null|\n", "2503.13200": "|**2025-03-17**|**Timing the Match: A Deep Reinforcement Learning Approach for Ride-Hailing and Ride-Pooling Services**|Yiman Bao et.al.|[2503.13200](http://arxiv.org/abs/2503.13200)|null|\n", "2503.13557": "|**2025-03-17**|**APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games**|Yifei Chen et.al.|[2503.13557](http://arxiv.org/abs/2503.13557)|null|\n", "2503.08388": "|**2025-03-11**|**V-Max: Making RL practical for Autonomous Driving**|Valentin Charraut et.al.|[2503.08388](http://arxiv.org/abs/2503.08388)|**[link](https://github.com/valeoai/v-max)**|\n", "2503.07433": "|**2025-04-22**|**DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent Networks**|Feiran You et.al.|[2503.07433](http://arxiv.org/abs/2503.07433)|**[link](https://github.com/nice-hku/dress)**|\n", "2503.05996": "|**2025-03-08**|**Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners**|Calarina Muslimani et.al.|[2503.05996](http://arxiv.org/abs/2503.05996)|null|\n", "2503.05226": "|**2025-03-07**|**Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments**|Xibai Wang et.al.|[2503.05226](http://arxiv.org/abs/2503.05226)|null|\n", "2503.04472": "|**2025-03-06**|**DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models**|Yi Shen et.al.|[2503.04472](http://arxiv.org/abs/2503.04472)|null|\n", "2503.00971": "|**2025-03-02**|**An Efficient and Uncertainty-aware Reinforcement Learning Framework for Quality Assurance in Extrusion Additive Manufacturing**|Xiaohan Li et.al.|[2503.00971](http://arxiv.org/abs/2503.00971)|null|\n", "2502.20265": "|**2025-03-03**|**On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+($\u03bb$,$\u03bb$))-GA**|Tai Nguyen et.al.|[2502.20265](http://arxiv.org/abs/2502.20265)|null|\n", "2502.19920": "|**2025-03-11**|**Pokemon Red via Reinforcement Learning**|Marco Pleines et.al.|[2502.19920](http://arxiv.org/abs/2502.19920)|**[link](https://github.com/MarcoMeter/neroRL)**|\n", "2502.18770": "|**2025-02-27**|**Reward Shaping to Mitigate Reward Hacking in RLHF**|Jiayi Fu et.al.|[2502.18770](http://arxiv.org/abs/2502.18770)|**[link](https://github.com/poruna-byte/par)**|\n", "2502.15922": "|**2025-02-21**|**On the Design of Safe Continual RL Methods for Control of Nonlinear Systems**|Austin Coursey et.al.|[2502.15922](http://arxiv.org/abs/2502.15922)|**[link](https://github.com/MACS-Research-Lab/safe-continual)**|\n", "2502.13430": "|**2025-02-19**|**Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning**|Hao Ma et.al.|[2502.13430](http://arxiv.org/abs/2502.13430)|null|\n", "2502.10325": "|**2025-02-14**|**Process Reward Models for LLM Agents: Practical Framework and Directions**|Sanjiban Choudhury et.al.|[2502.10325](http://arxiv.org/abs/2502.10325)|**[link](https://github.com/sanjibanc/agent_prm)**|\n", "2502.08643": "|**2025-02-18**|**A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards**|Shivansh Patel et.al.|[2502.08643](http://arxiv.org/abs/2502.08643)|null|\n", "2502.05069": "|**2025-02-07**|**Exploring the Generalizability of Geomagnetic Navigation: A Deep Reinforcement Learning approach with Policy Distillation**|Wenqi Bai et.al.|[2502.05069](http://arxiv.org/abs/2502.05069)|null|\n", "2502.04864": "|**2025-02-07**|**$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning**|Aditya Kapoor et.al.|[2502.04864](http://arxiv.org/abs/2502.04864)|null|\n", "2502.03373": "|**2025-02-05**|**Demystifying Long Chain-of-Thought Reasoning in LLMs**|Edward Yeo et.al.|[2502.03373](http://arxiv.org/abs/2502.03373)|**[link](https://github.com/eddycmu/demystify-long-cot)**|\n", "2502.02060": "|**2025-02-04**|**CH-MARL: Constrained Hierarchical Multiagent Reinforcement Learning for Sustainable Maritime Logistics**|Saad Alqithami et.al.|[2502.02060](http://arxiv.org/abs/2502.02060)|null|\n", "2502.01971": "|**2025-02-04**|**Bottom-Up Reputation Promotes Cooperation with Multi-Agent Reinforcement Learning**|Tianyu Ren et.al.|[2502.01971](http://arxiv.org/abs/2502.01971)|**[link](https://github.com/itstyren/lr2)**|\n", "2502.01307": "|**2025-02-03**|**Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning**|Henrik M\u00fcller et.al.|[2502.01307](http://arxiv.org/abs/2502.01307)|null|\n", "2502.00835": "|**2025-04-28**|**CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation**|Yuanchen Yuan et.al.|[2502.00835](http://arxiv.org/abs/2502.00835)|null|\n", "2501.19206": "|**2025-01-31**|**An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents**|Gregory Palmer et.al.|[2501.19206](http://arxiv.org/abs/2501.19206)|null|\n", "2501.19128": "|**2025-01-31**|**Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach**|Wenyun Li et.al.|[2501.19128](http://arxiv.org/abs/2501.19128)|null|\n", "2501.18858": "|**2025-01-31**|**BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning**|Han Zhong et.al.|[2501.18858](http://arxiv.org/abs/2501.18858)|null|\n", "2501.15971": "|**2025-01-27**|**REINFORCE-ING Chemical Language Models in Drug Design**|Morgan Thomas et.al.|[2501.15971](http://arxiv.org/abs/2501.15971)|null|\n", "2501.13727": "|**2025-04-01**|**Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System**|Haikuo Du et.al.|[2501.13727](http://arxiv.org/abs/2501.13727)|**[link](https://github.com/qeneb/ss-marl)**|\n", "2501.12627": "|**2025-01-22**|**Deep Reinforcement Learning with Hybrid Intrinsic Reward Model**|Mingqi Yuan et.al.|[2501.12627](http://arxiv.org/abs/2501.12627)|null|\n", "2501.10938": "|**2025-01-19**|**Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning**|Ahmed Alagha et.al.|[2501.10938](http://arxiv.org/abs/2501.10938)|null|\n", "2504.19473": "|**2025-01-18**|**Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function**|Donghe Chen et.al.|[2504.19473](http://arxiv.org/abs/2504.19473)|null|\n", "2501.07445": "|**2025-01-13**|**Online inductive learning from answer sets for efficient reinforcement learning exploration**|Celeste Veronese et.al.|[2501.07445](http://arxiv.org/abs/2501.07445)|null|\n", "2501.03884": "|**2025-02-20**|**AlphaPO -- Reward shape matters for LLM alignment**|Aman Gupta et.al.|[2501.03884](http://arxiv.org/abs/2501.03884)|null|\n", "2505.07257": "|**2025-05-12**|**DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward**|Yi Zhang et.al.|[2505.07257](http://arxiv.org/abs/2505.07257)|null|\n", "2505.06744": "|**2025-05-10**|**LineFlow: A Framework to Learn Active Control of Production Lines**|Kai M\u00fcller et.al.|[2505.06744](http://arxiv.org/abs/2505.06744)|**[link](https://github.com/hs-kempten/lineflow)**|\n", "2505.12611": "|**2025-05-19**|**Action-Dependent Optimality-Preserving Reward Shaping**|Grant C. Forbes et.al.|[2505.12611](http://arxiv.org/abs/2505.12611)|null|\n", "2505.11661": "|**2025-05-16**|**Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning**|Zihan Ye et.al.|[2505.11661](http://arxiv.org/abs/2505.11661)|null|\n", "2505.11478": "|**2025-05-16**|**Automatic Reward Shaping from Confounded Offline Data**|Mingxuan Li et.al.|[2505.11478](http://arxiv.org/abs/2505.11478)|null|\n", "2505.10832": "|**2025-05-28**|**Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL**|Songjun Tu et.al.|[2505.10832](http://arxiv.org/abs/2505.10832)|**[link](https://github.com/tu2021/autothink)**|\n", "2505.10802": "|**2025-05-16**|**Attention-Based Reward Shaping for Sparse and Delayed Rewards**|Ian Holmes et.al.|[2505.10802](http://arxiv.org/abs/2505.10802)|**[link](https://github.com/ihholmes-p/ares)**|\n", "2505.16217": "|**2025-05-22**|**Reward-Aware Proto-Representations in Reinforcement Learning**|Hon Tik Tse et.al.|[2505.16217](http://arxiv.org/abs/2505.16217)|null|\n", "2505.15922": "|**2025-05-21**|**Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition**|Dong Won Lee et.al.|[2505.15922](http://arxiv.org/abs/2505.15922)|null|\n", "2505.15776": "|**2025-05-21**|**ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning**|Changtai Zhu et.al.|[2505.15776](http://arxiv.org/abs/2505.15776)|**[link](https://github.com/BeastyZ/ConvSearch-R1)**|\n", "2505.15612": "|**2025-05-21**|**Learn to Reason Efficiently with Adaptive Length-based Reward Shaping**|Wei Liu et.al.|[2505.15612](http://arxiv.org/abs/2505.15612)|**[link](https://github.com/hkust-nlp/laser)**|\n", "2505.15515": "|**2025-05-21**|**From learning to safety: A Direct Data-Driven Framework for Constrained Control**|Kanghui He et.al.|[2505.15515](http://arxiv.org/abs/2505.15515)|null|\n", "2505.14157": "|**2025-05-20**|**Prior Prompt Engineering for Reinforcement Fine-Tuning**|Pittawat Taveekitworachai et.al.|[2505.14157](http://arxiv.org/abs/2505.14157)|null|\n", "2505.13925": "|**2025-05-20**|**Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning**|Yunpeng Jiang et.al.|[2505.13925](http://arxiv.org/abs/2505.13925)|null|\n", "2505.23355": "|**2025-05-29**|**Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control**|Maxiu Xiao et.al.|[2505.23355](http://arxiv.org/abs/2505.23355)|null|\n", "2505.20218": "|**2025-05-26**|**Fine-grained List-wise Alignment for Generative Medication Recommendation**|Chenxiao Fan et.al.|[2505.20218](http://arxiv.org/abs/2505.20218)|**[link](https://github.com/cxfann/flame)**|\n", "2505.19196": "|**2025-05-25**|**Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning**|Xinyao Liao et.al.|[2505.19196](http://arxiv.org/abs/2505.19196)|**[link](https://github.com/lil-shake/coca)**|\n", "2505.18994": "|**2025-05-25**|**Designing Pin-pression Gripper and Learning its Dexterous Grasping with Online In-hand Adjustment**|Hewen Xiao et.al.|[2505.18994](http://arxiv.org/abs/2505.18994)|**[link](https://github.com/siggraph-pin-pression-gripper/pin-pression-gripper-video)**|\n", "2505.18417": "|**2025-05-23**|**Reinforcement Learning for Ballbot Navigation in Uneven Terrain**|Achkan Salehi et.al.|[2505.18417](http://arxiv.org/abs/2505.18417)|**[link](https://github.com/salehiac/openballbot-rl)**|\n", "2505.18298": "|**2025-05-23**|**Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards**|Jinyan Su et.al.|[2505.18298](http://arxiv.org/abs/2505.18298)|null|\n", "2505.20315": "|**2025-05-22**|**Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL**|Zhewei Yao et.al.|[2505.20315](http://arxiv.org/abs/2505.20315)|null|\n"}, "Potential Funtions": {"nlin/0112040": "|**2002-12-19**|**DNA Torsional Solitons in Presence of localized Inhomogeneities**|Luis Morales Molina et.al.|[nlin/0112040](http://arxiv.org/abs/nlin/0112040)|null|\n"}, "Potential Funtion": {"nlin/0112040": "|**2002-12-19**|**DNA Torsional Solitons in Presence of localized Inhomogeneities**|Luis Morales Molina et.al.|[nlin/0112040](http://arxiv.org/abs/nlin/0112040)|null|\n"}, "State representation learning": {"2502.02327": "|**2025-02-04**|**Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation**|Siyu Wang et.al.|[2502.02327](http://arxiv.org/abs/2502.02327)|null|\n", "2410.15979": "|**2025-03-06**|**Learning Quadrotor Control From Visual Features Using Differentiable Simulation**|Johannes Heeg et.al.|[2410.15979](http://arxiv.org/abs/2410.15979)|**[link](https://github.com/uzh-rpg/rpg_flightning)**|\n", "2409.13824": "|**2025-03-11**|**Adaptive Task Allocation in Multi-Human Multi-Robot Teams under Team Heterogeneity and Dynamic Information Uncertainty**|Ziqin Yuan et.al.|[2409.13824](http://arxiv.org/abs/2409.13824)|null|\n", "2407.13091": "|**2024-07-18**|**On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems**|Siyu Wang et.al.|[2407.13091](http://arxiv.org/abs/2407.13091)|null|\n", "2406.14476": "|**2024-07-16**|**Learning telic-controllable state representations**|Nadav Amir et.al.|[2406.14476](http://arxiv.org/abs/2406.14476)|null|\n", "2405.13848": "|**2024-05-22**|**Maximum Manifold Capacity Representations in State Representation Learning**|Li Meng et.al.|[2405.13848](http://arxiv.org/abs/2405.13848)|null|\n", "2312.02367": "|**2024-01-04**|**States as goal-directed concepts: an epistemic approach to state-representation learning**|Nadav Amir et.al.|[2312.02367](http://arxiv.org/abs/2312.02367)|null|\n", "2312.10053": "|**2023-12-03**|**Towards Goal-oriented Intelligent Tutoring Systems in Online Education**|Yang Deng et.al.|[2312.10053](http://arxiv.org/abs/2312.10053)|null|\n", "2310.14274": "|**2023-10-22**|**Robust Visual Imitation Learning with Inverse Dynamics Representations**|Siyuan Li et.al.|[2310.14274](http://arxiv.org/abs/2310.14274)|null|\n", "2310.04241": "|**2023-10-09**|**Improving Reinforcement Learning Efficiency with Auxiliary Tasks in Non-Visual Environments: A Comparison**|Moritz Lange et.al.|[2310.04241](http://arxiv.org/abs/2310.04241)|null|\n", "2308.14556": "|**2023-12-15**|**On learning latent dynamics of the AUG plasma state**|A. Kit et.al.|[2308.14556](http://arxiv.org/abs/2308.14556)|**[link](https://github.com/digifusion/latent-state-modeling)**|\n", "2306.11197": "|**2023-11-04**|**Sparse Modular Activation for Efficient Sequence Modeling**|Liliang Ren et.al.|[2306.11197](http://arxiv.org/abs/2306.11197)|**[link](https://github.com/renll/seqboat)**|\n", "2305.11081": "|**2023-05-18**|**Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems**|Zhaochun Ren et.al.|[2305.11081](http://arxiv.org/abs/2305.11081)|**[link](https://github.com/hn-rs/csa)**|\n", "2305.10267": "|**2024-06-24**|**State Representation Learning Using an Unbalanced Atlas**|Li Meng et.al.|[2305.10267](http://arxiv.org/abs/2305.10267)|**[link](https://github.com/mengli11235/dim-ua)**|\n", "2305.02968": "|**2023-05-04**|**Masked Trajectory Models for Prediction, Representation, and Control**|Philipp Wu et.al.|[2305.02968](http://arxiv.org/abs/2305.02968)|**[link](https://github.com/facebookresearch/mtm)**|\n", "2303.07437": "|**2023-03-13**|**Unsupervised Representation Learning in Partially Observable Atari Games**|Li Meng et.al.|[2303.07437](http://arxiv.org/abs/2303.07437)|**[link](https://github.com/mengli11235/mst_dim)**|\n", "2212.14511": "|**2024-03-13**|**Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?**|Yi Tian et.al.|[2212.14511](http://arxiv.org/abs/2212.14511)|null|\n", "2209.05302": "|**2022-09-12**|**Unified State Representation Learning under Data Augmentation**|Taylor Hearn et.al.|[2209.05302](http://arxiv.org/abs/2209.05302)|**[link](https://github.com/SVJayanthi/dmcontrol-generalization-benchmark)**|\n", "2205.01965": "|**2022-05-04**|**State Representation Learning for Goal-Conditioned Reinforcement Learning**|Lorenzo Steccanella et.al.|[2205.01965](http://arxiv.org/abs/2205.01965)|null|\n", "2202.02881": "|**2022-11-14**|**Approximate Policy Iteration with Bisimulation Metrics**|Mete Kemertas et.al.|[2202.02881](http://arxiv.org/abs/2202.02881)|**[link](https://github.com/metekemertas/api-bisim)**|\n", "2201.12096": "|**2022-10-09**|**Mask-based Latent Reconstruction for Reinforcement Learning**|Tao Yu et.al.|[2201.12096](http://arxiv.org/abs/2201.12096)|**[link](https://github.com/microsoft/Mask-based-Latent-Reconstruction)**|\n", "2201.07016": "|**2022-01-18**|**Accelerating Representation Learning with View-Consistent Dynamics in Data-Efficient Reinforcement Learning**|Tao Huang et.al.|[2201.07016](http://arxiv.org/abs/2201.07016)|null|\n", "2110.12352": "|**2022-07-26**|**DiffSRL: Learning Dynamical State Representation for Deformable Object Manipulation with Differentiable Simulator**|Sirui Chen et.al.|[2110.12352](http://arxiv.org/abs/2110.12352)|**[link](https://github.com/ericcsr/diffsrl)**|\n", "2110.05721": "|**2022-06-19**|**Action-Sufficient State Representation Learning for Control with Structural Constraints**|Biwei Huang et.al.|[2110.05721](http://arxiv.org/abs/2110.05721)|null|\n", "2110.00784": "|**2021-10-02**|**Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning**|Elie Aljalbout et.al.|[2110.00784](http://arxiv.org/abs/2110.00784)|null|\n", "2109.13596": "|**2022-02-15**|**Exploratory State Representation Learning**|Astrid Merckling et.al.|[2109.13596](http://arxiv.org/abs/2109.13596)|**[link](https://github.com/astrid-merckling/SRL4RL)**|\n", "2109.13588": "|**2021-09-28**|**Making Curiosity Explicit in Vision-based RL**|Elie Aljalbout et.al.|[2109.13588](http://arxiv.org/abs/2109.13588)|null|\n", "2109.08642": "|**2023-12-09**|**POAR: Efficient Policy Optimization via Online Abstract State Representation Learning**|Zhaorun Chen et.al.|[2109.08642](http://arxiv.org/abs/2109.08642)|**[link](https://github.com/billchan226/poar-srl-4-robot)**|\n", "2109.06737": "|**2021-09-14**|**Comparing Reconstruction- and Contrastive-based Models for Visual Task Planning**|Constantinos Chamzas et.al.|[2109.06737](http://arxiv.org/abs/2109.06737)|null|\n", "2107.01667": "|**2021-07-04**|**Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces**|Nicol\u00f2 Botteghi et.al.|[2107.01667](http://arxiv.org/abs/2107.01667)|null|\n", "2106.14838": "|**2021-06-28**|**Improving Prediction of Low-Prior Clinical Events with Simultaneous General Patient-State Representation Learning**|Matthew Barren et.al.|[2106.14838](http://arxiv.org/abs/2106.14838)|null|\n", "2106.05139": "|**2021-06-09**|**Pretrained Encoders are All You Need**|Mina Khan et.al.|[2106.05139](http://arxiv.org/abs/2106.05139)|**[link](https://github.com/PAL-ML/PEARL_v1)**|\n", "2103.04412": "|**2021-03-07**|**Multimodal VAE Active Inference Controller**|Cristian Meo et.al.|[2103.04412](http://arxiv.org/abs/2103.04412)|**[link](https://github.com/Cmeo97/MAIF)**|\n", "2103.04351": "|**2021-03-07**|**Learning a State Representation and Navigation in Cluttered and Dynamic Environments**|David Hoeller et.al.|[2103.04351](http://arxiv.org/abs/2103.04351)|null|\n", "2102.04897": "|**2021-11-05**|**Learning State Representations from Random Deep Action-conditional Predictions**|Zeyu Zheng et.al.|[2102.04897](http://arxiv.org/abs/2102.04897)|**[link](https://github.com/Hwhitetooth/random_gvfs)**|\n", "2007.16044": "|**2021-01-07**|**Low Dimensional State Representation Learning with Reward-shaped Priors**|Nicol\u00f2 Botteghi et.al.|[2007.16044](http://arxiv.org/abs/2007.16044)|null|\n", "2005.06369": "|**2020-05-13**|**Progressive growing of self-organized hierarchical representations for exploration**|Mayalen Etcheverry et.al.|[2005.06369](http://arxiv.org/abs/2005.06369)|null|\n", "2004.13965": "|**2021-02-16**|**Graph-based State Representation for Deep Reinforcement Learning**|Vikram Waradpande et.al.|[2004.13965](http://arxiv.org/abs/2004.13965)|null|\n", "2002.11903": "|**2020-02-27**|**Acceleration of Actor-Critic Deep Reinforcement Learning for Visual Grasping in Clutter by State Representation Learning Based on Disentanglement of a Raw Input Image**|Taewon Kim et.al.|[2002.11903](http://arxiv.org/abs/2002.11903)|null|\n", "2001.11628": "|**2021-06-04**|**Domain-Adversarial and Conditional State Space Model for Imitation Learning**|Ryo Okumura et.al.|[2001.11628](http://arxiv.org/abs/2001.11628)|null|\n"}}