---
layout: default
---

## Updated on 2025.05.07
> Usage instructions: [here](./docs/README.md#usage)

## MuZero

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-20**|**AlphaZero-Edu: Making AlphaZero Accessible to Everyone**|Binjie Guo et.al.|[2504.14636](http://arxiv.org/abs/2504.14636)|**[link](https://github.com/starlight1212/alphazero_edu)**|
|**2025-04-10**|**Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency**|Ameya Joshi et.al.|[2504.07757](http://arxiv.org/abs/2504.07757)|null|
|**2025-04-09**|**AssistanceZero: Scalably Solving Assistance Games**|Cassidy Laidlaw et.al.|[2504.07091](http://arxiv.org/abs/2504.07091)|**[link](https://github.com/cassidylaidlaw/minecraft-building-assistance-game)**|
|**2025-04-07**|**A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%**|Benny Skidanov et.al.|[2504.05425](http://arxiv.org/abs/2504.05425)|null|
|**2025-03-26**|**World Model Agents with Change-Based Intrinsic Motivation**|Jeremias Ferrao et.al.|[2503.21047](http://arxiv.org/abs/2503.21047)|**[link](https://github.com/Jazhyc/world-model-policy-transfer)**|
|**2025-03-17**|**Rapfi: Distilling Efficient Neural Network for the Game of Gomoku**|Zhanggen Jin et.al.|[2503.13178](http://arxiv.org/abs/2503.13178)|null|
|**2025-05-05**|**Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science**|Johannes Buchner et.al.|[2503.10822](http://arxiv.org/abs/2503.10822)|null|
|**2025-03-11**|**Meta-Reinforcement Learning with Discrete World Models for Adaptive Load Balancing**|Cameron Redovian et.al.|[2503.08872](http://arxiv.org/abs/2503.08872)|null|
|**2025-02-20**|**Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective**|Krti Tallam et.al.|[2503.05748](http://arxiv.org/abs/2503.05748)|null|
|**2025-03-07**|**InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model**|Feeza Khan Khanzada et.al.|[2503.05573](http://arxiv.org/abs/2503.05573)|null|
|**2025-03-06**|**Learning Transformer-based World Models with Contrastive Predictive Coding**|Maxime Burchi et.al.|[2503.04416](http://arxiv.org/abs/2503.04416)|null|
|**2025-03-04**|**DreamerV3 for Traffic Signal Control: Hyperparameter Tuning and Performance**|Qiang Li et.al.|[2503.02279](http://arxiv.org/abs/2503.02279)|null|
|**2025-03-01**|**Discrete Codebook World Models for Continuous Control**|Aidan Scannell et.al.|[2503.00653](http://arxiv.org/abs/2503.00653)|**[link](https://github.com/aidanscannell/dcmpc)**|
|**2025-03-21**|**OptionZero: Planning with Learned Options**|Po-Wei Huang et.al.|[2502.16634](http://arxiv.org/abs/2502.16634)|**[link](https://github.com/rlglab/optionzero)**|
|**2025-02-17**|**TSS GAZ PTP: Towards Improving Gumbel AlphaZero with Two-stage Self-play for Multi-constrained Electric Vehicle Routing Problems**|Hui Wang et.al.|[2502.15777](http://arxiv.org/abs/2502.15777)|null|
|**2025-02-19**|**Playing Hex and Counter Wargames using Reinforcement Learning and Recurrent Neural Networks**|Guilherme Palma et.al.|[2502.13918](http://arxiv.org/abs/2502.13918)|**[link](https://github.com/guilherme439/nuzero)**|
|**2025-02-14**|**Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations**|Abdelrhman Shaheen et.al.|[2502.10303](http://arxiv.org/abs/2502.10303)|null|
|**2025-02-10**|**On the Emergence of Thinking in LLMs I: Searching for the Right Intuition**|Guanghao Ye et.al.|[2502.06773](http://arxiv.org/abs/2502.06773)|**[link](https://github.com/GuanghaoYe/Emergence-of-Thinking)**|
|**2025-02-08**|**Efficient Reinforcement Learning Through Adaptively Pretrained Visual Encoder**|Yuhan Zhang et.al.|[2502.05555](http://arxiv.org/abs/2502.05555)|null|
|**2025-02-03**|**Improving Transformer World Models for Data-Efficient RL**|Antoine Dedieu et.al.|[2502.01591](http://arxiv.org/abs/2502.01591)|null|
|**2025-01-24**|**Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone Flight**|Angel Romero et.al.|[2501.14377](http://arxiv.org/abs/2501.14377)|null|
|**2024-12-23**|**Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning**|Huchen Jiang et.al.|[2412.17397](http://arxiv.org/abs/2412.17397)|null|
|**2025-03-08**|**What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning**|Yiran Ma et.al.|[2412.15904](http://arxiv.org/abs/2412.15904)|null|
|**2024-12-16**|**AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws**|Oren Neumann et.al.|[2412.11979](http://arxiv.org/abs/2412.11979)|**[link](https://github.com/orenneumann/alphazero_zipfs_law)**|
|**2024-12-08**|**Policy-shaped prediction: avoiding distractions in model-based reinforcement learning**|Miles Hutson et.al.|[2412.05766](http://arxiv.org/abs/2412.05766)|null|
|**2024-11-13**|**Evaluating World Models with LLM for Decision Making**|Chang Yang et.al.|[2411.08794](http://arxiv.org/abs/2411.08794)|null|
|**2024-11-10**|**Mastering NIM and Impartial Games with Weak Neural Networks: An AlphaZero-inspired Multi-Frame Approach**|Søren Riis et.al.|[2411.06403](http://arxiv.org/abs/2411.06403)|null|
|**2024-11-07**|**Evaluating Robustness of Reinforcement Learning Algorithms for Autonomous Shipping**|Bavo Lesy et.al.|[2411.04915](http://arxiv.org/abs/2411.04915)|null|
|**2024-11-07**|**Interpreting the Learned Model in MuZero Planning**|Hung Guei et.al.|[2411.04580](http://arxiv.org/abs/2411.04580)|null|
|**2024-10-31**|**Enhancing Chess Reinforcement Learning with Graph Representation**|Tomas Rigaux et.al.|[2410.23753](http://arxiv.org/abs/2410.23753)|**[link](https://github.com/akulen/alphagateau)**|
|**2024-10-18**|**Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments**|Mariusz Wisniewski et.al.|[2410.14616](http://arxiv.org/abs/2410.14616)|**[link](https://github.com/mazqtpopx/cranfield-navigation-gym)**|
|**2024-10-15**|**Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning**|Jiayu Chen et.al.|[2410.11234](http://arxiv.org/abs/2410.11234)|null|
|**2025-02-07**|**SOLD: Slot Object-Centric Latent Dynamics Models for Relational Manipulation Learning from Pixels**|Malte Mosbach et.al.|[2410.08822](http://arxiv.org/abs/2410.08822)|null|
|**2024-10-07**|**ResTNet: Defense against Adversarial Policies via Transformer in Computer Go**|Tai-Lin Wu et.al.|[2410.05347](http://arxiv.org/abs/2410.05347)|null|
|**2024-10-31**|**Maia-2: A Unified Model for Human-AI Alignment in Chess**|Zhenwei Tang et.al.|[2409.20553](http://arxiv.org/abs/2409.20553)|**[link](https://github.com/csslab/maia2)**|
|**2025-03-18**|**MARLadona -- Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning**|Zichong Li et.al.|[2409.20326](http://arxiv.org/abs/2409.20326)|null|
|**2024-10-28**|**Mastering Chess with a Transformer Model**|Daniel Monroe et.al.|[2409.12272](http://arxiv.org/abs/2409.12272)|**[link](https://github.com/ergodice/lczero-training)**|
|**2024-08-27**|**Enhancing Analogical Reasoning in the Abstraction and Reasoning Corpus via Model-Based RL**|Jihwan Lee et.al.|[2408.14855](http://arxiv.org/abs/2408.14855)|null|
|**2024-11-29**|**AlphaViT: A Flexible Game-Playing AI for Multiple Games and Variable Board Sizes**|Kazuhisa Fujita et.al.|[2408.13871](http://arxiv.org/abs/2408.13871)|**[link](https://github.com/kazuhisafujita/alphavit)**|
|**2024-10-02**|**ShortCircuit: AlphaZero-Driven Circuit Design**|Dimitrios Tsaras et.al.|[2408.09858](http://arxiv.org/abs/2408.09858)|null|

## Game

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-06**|**Chess variation entropy and engine relevance for humans**|Marc Barthelemy et.al.|[2505.03251](http://arxiv.org/abs/2505.03251)|null|
|**2025-05-01**|**Policies of Multiple Skill Levels for Better Strength Estimation in Games**|Kyota Kuboki et.al.|[2505.00279](http://arxiv.org/abs/2505.00279)|null|
|**2025-04-30**|**Leveraging Systems and Control Theory for Social Robotics: A Model-Based Behavioral Control Approach to Human-Robot Interaction**|Maria Morão Patrício et.al.|[2504.21548](http://arxiv.org/abs/2504.21548)|null|
|**2025-04-19**|**Research Power Ranking: Adapting the Elo System to Quantify Scientist Evaluation**|Eldar Knar et.al.|[2504.20061](http://arxiv.org/abs/2504.20061)|null|
|**2025-04-24**|**Do We Need Transformers to Play FPS Video Games?**|Karmanbir Batth et.al.|[2504.17891](http://arxiv.org/abs/2504.17891)|null|
|**2025-04-20**|**AlphaZero-Edu: Making AlphaZero Accessible to Everyone**|Binjie Guo et.al.|[2504.14636](http://arxiv.org/abs/2504.14636)|**[link](https://github.com/starlight1212/alphazero_edu)**|
|**2025-04-18**|**SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents**|Avaneesh Devkota et.al.|[2504.13541](http://arxiv.org/abs/2504.13541)|null|
|**2025-04-17**|**Evolutionary Policy Optimization**|Zelal Su "Lain" Mustafaoglu et.al.|[2504.12568](http://arxiv.org/abs/2504.12568)|null|
|**2025-04-17**|**ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition**|Haidar Khan et.al.|[2504.12562](http://arxiv.org/abs/2504.12562)|**[link](https://github.com/facebookresearch/zerosumeval)**|
|**2025-04-16**|**pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild**|Jonas Myhre Schiøtt et.al.|[2504.12045](http://arxiv.org/abs/2504.12045)|null|
|**2025-04-15**|**Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay**|Henrik Krauss et.al.|[2504.11118](http://arxiv.org/abs/2504.11118)|null|
|**2025-04-14**|**Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning**|Tien Pham et.al.|[2504.10071](http://arxiv.org/abs/2504.10071)|**[link](https://github.com/tiencapham/ife)**|
|**2025-04-22**|**Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning**|Zhiyong Wang et.al.|[2504.09192](http://arxiv.org/abs/2504.09192)|null|
|**2025-04-09**|**Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning**|Jiahua Lan et.al.|[2504.08000](http://arxiv.org/abs/2504.08000)|null|
|**2025-04-10**|**Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency**|Ameya Joshi et.al.|[2504.07757](http://arxiv.org/abs/2504.07757)|null|
|**2025-04-09**|**Better Decisions through the Right Causal World Model**|Elisabeth Dillies et.al.|[2504.07257](http://arxiv.org/abs/2504.07257)|null|
|**2025-03-26**|**UAV-Assisted MEC for Disaster Response: Stackelberg Game-Based Resource Optimization**|Yafei Guo et.al.|[2504.07119](http://arxiv.org/abs/2504.07119)|null|
|**2025-04-08**|**Diagrammatic expansion for the mutual-information rate in the realm of limited statistics**|Tobias Kühn et.al.|[2504.06255](http://arxiv.org/abs/2504.06255)|null|
|**2025-04-08**|**Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments**|Dolton Fernandes et.al.|[2504.05840](http://arxiv.org/abs/2504.05840)|null|
|**2025-04-07**|**A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%**|Benny Skidanov et.al.|[2504.05425](http://arxiv.org/abs/2504.05425)|null|
|**2025-04-07**|**Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot Motion**|Eran Beeri Bamani et.al.|[2504.05084](http://arxiv.org/abs/2504.05084)|null|
|**2025-04-07**|**Playing Non-Embedded Card-Based Games with Reinforcement Learning**|Tianyang Wu et.al.|[2504.04783](http://arxiv.org/abs/2504.04783)|**[link](https://github.com/wty-yy/katacr)**|
|**2025-04-03**|**Deep Reinforcement Learning via Object-Centric Attention**|Jannis Blüml et.al.|[2504.03024](http://arxiv.org/abs/2504.03024)|**[link](https://github.com/VanillaWhey/OCAtariWrappers)**|
|**2025-03-30**|**Quantum Methods for Managing Ambiguity in Natural Language Processing**|Jurek Eisinger et.al.|[2504.00040](http://arxiv.org/abs/2504.00040)|**[link](https://github.com/jurekjurek/managingambiguity)**|
|**2025-03-27**|**Investigating Experiential Effects in Online Chess using a Hierarchical Bayesian Analysis**|Adam Gee et.al.|[2503.21713](http://arxiv.org/abs/2503.21713)|null|
|**2025-03-27**|**LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning**|Hui Wang et.al.|[2503.21683](http://arxiv.org/abs/2503.21683)|null|
|**2025-03-27**|**DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset**|Ling Feng et.al.|[2503.21323](http://arxiv.org/abs/2503.21323)|null|
|**2025-04-22**|**MAD Chairs: A new tool to evaluate AI**|Chris Santos-Lang et.al.|[2503.20986](http://arxiv.org/abs/2503.20986)|null|
|**2025-03-26**|**Look Before Leap: Look-Ahead Planning with Uncertainty in Reinforcement Learning**|Yongshuai Liu et.al.|[2503.20139](http://arxiv.org/abs/2503.20139)|null|
|**2025-03-24**|**Leaper Embeddings**|Nikolai Beluhov et.al.|[2503.18700](http://arxiv.org/abs/2503.18700)|null|
|**2025-03-24**|**Adventurer: Exploration with BiGAN for Deep Reinforcement Learning**|Yongshuai Liu et.al.|[2503.18612](http://arxiv.org/abs/2503.18612)|null|
|**2025-03-17**|**APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games**|Yifei Chen et.al.|[2503.13557](http://arxiv.org/abs/2503.13557)|null|
|**2025-03-17**|**Rapfi: Distilling Efficient Neural Network for the Game of Gomoku**|Zhanggen Jin et.al.|[2503.13178](http://arxiv.org/abs/2503.13178)|null|
|**2025-03-17**|**Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models**|Zhiyi Huang et.al.|[2503.12923](http://arxiv.org/abs/2503.12923)|null|
|**2025-03-17**|**Verification Learning: Make Unsupervised Neuro-Symbolic System Feasible**|Lin-Han Jia et.al.|[2503.12917](http://arxiv.org/abs/2503.12917)|null|
|**2025-05-05**|**Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science**|Johannes Buchner et.al.|[2503.10822](http://arxiv.org/abs/2503.10822)|null|
|**2025-04-17**|**ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition**|Hisham A. Alyahya et.al.|[2503.10673](http://arxiv.org/abs/2503.10673)|**[link](https://github.com/zerosumeval/zerosumeval)**|
|**2025-03-12**|**StratIncon Detector: Analyzing Strategy Inconsistencies Between Real-Time Strategy and Preferred Professional Strategy in MOBA Esports**|Ruofei Ma et.al.|[2503.09060](http://arxiv.org/abs/2503.09060)|null|
|**2025-03-05**|**Mixture of Experts Made Intrinsically Interpretable**|Xingyi Yang et.al.|[2503.07639](http://arxiv.org/abs/2503.07639)|null|
|**2025-03-06**|**Learning Transformer-based World Models with Contrastive Predictive Coding**|Maxime Burchi et.al.|[2503.04416](http://arxiv.org/abs/2503.04416)|null|

## Doudizhu

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Quantum Entanglement Autodistillation in Baryon Pair Decays**|Hai-Long Feng et.al.|[2504.15798](http://arxiv.org/abs/2504.15798)|null|
|**2025-03-18**|**Empowering LLMs in Decision Games through Algorithmic Data Synthesis**|Haolin Wang et.al.|[2503.13980](http://arxiv.org/abs/2503.13980)|null|
|**2024-09-13**|**AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding**|Chang Lei et.al.|[2407.10279](http://arxiv.org/abs/2407.10279)|**[link](https://github.com/RuBP17/AlphaDou)**|
|**2024-03-21**|**DouRN: Improving DouZero by Residual Neural Networks**|Yiquan Chen et.al.|[2403.14102](http://arxiv.org/abs/2403.14102)|null|
|**2023-12-05**|**DanZero+: Dominating the GuanDan Game through Reinforcement Learning**|Youpeng Zhao et.al.|[2312.02561](http://arxiv.org/abs/2312.02561)|**[link](https://github.com/submit-paper/Danzero_plus)**|
|**2022-10-31**|**DanZero: Mastering GuanDan Game with Reinforcement Learning**|Yudong Lu et.al.|[2210.17087](http://arxiv.org/abs/2210.17087)|null|
|**2022-04-06**|**DouZero+: Improving DouDizhu AI by Opponent Modeling and Coach-guided Learning**|Youpeng Zhao et.al.|[2204.02558](http://arxiv.org/abs/2204.02558)|**[link](https://github.com/submit-paper/doudizhu)**|
|**2024-02-28**|**PerfectDou: Dominating DouDizhu with Perfect Information Distillation**|Guan Yang et.al.|[2203.16406](http://arxiv.org/abs/2203.16406)|**[link](https://github.com/netease-games-ai-lab-guangzhou/perfectdou)**|
|**2021-06-11**|**DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning**|Daochen Zha et.al.|[2106.06135](http://arxiv.org/abs/2106.06135)|**[link](https://github.com/kwai/DouZero)**|
|**2012-01-18**|**Electric dipole polarizability and the neutron skin**|J. Piekarewicz et.al.|[1201.3807](http://arxiv.org/abs/1201.3807)|null|

## Reward Shaping

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-01-18**|**Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function**|Donghe Chen et.al.|[2504.19473](http://arxiv.org/abs/2504.19473)|null|
|**2025-04-22**|**Learning Explainable Dense Reward Shapes via Bayesian Optimization**|Ryan Koo et.al.|[2504.16272](http://arxiv.org/abs/2504.16272)|null|
|**2025-04-22**|**Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration**|Tyler Ga Wei Lum et.al.|[2504.12609](http://arxiv.org/abs/2504.12609)|null|
|**2025-04-16**|**Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime**|Thorben Markmann et.al.|[2504.12000](http://arxiv.org/abs/2504.12000)|null|
|**2025-04-14**|**Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning**|Muhammad Al-Zafar Khan et.al.|[2504.10677](http://arxiv.org/abs/2504.10677)|null|
|**2025-04-14**|**Reasoning without Regret**|Tarun Chitra et.al.|[2504.09777](http://arxiv.org/abs/2504.09777)|null|
|**2025-03-26**|**Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping**|Mohammad Saif Nazir et.al.|[2503.22723](http://arxiv.org/abs/2503.22723)|**[link](https://github.com/RizanSM/zero_shot_llms_in_HIL_RL)**|
|**2025-03-27**|**Reward Design for Reinforcement Learning Agents**|Rati Devidze et.al.|[2503.21949](http://arxiv.org/abs/2503.21949)|**[link](https://github.com/adishs/neurips2021_explicable-reward-design_code)**|
|**2025-03-23**|**ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data**|Nitish Dashora et.al.|[2503.18210](http://arxiv.org/abs/2503.18210)|null|
|**2025-03-17**|**APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games**|Yifei Chen et.al.|[2503.13557](http://arxiv.org/abs/2503.13557)|null|
|**2025-03-17**|**Timing the Match: A Deep Reinforcement Learning Approach for Ride-Hailing and Ride-Pooling Services**|Yiman Bao et.al.|[2503.13200](http://arxiv.org/abs/2503.13200)|null|
|**2025-03-11**|**V-Max: Making RL practical for Autonomous Driving**|Valentin Charraut et.al.|[2503.08388](http://arxiv.org/abs/2503.08388)|**[link](https://github.com/valeoai/v-max)**|
|**2025-04-22**|**DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent Networks**|Feiran You et.al.|[2503.07433](http://arxiv.org/abs/2503.07433)|**[link](https://github.com/nice-hku/dress)**|
|**2025-03-08**|**Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners**|Calarina Muslimani et.al.|[2503.05996](http://arxiv.org/abs/2503.05996)|null|
|**2025-03-07**|**Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments**|Xibai Wang et.al.|[2503.05226](http://arxiv.org/abs/2503.05226)|null|
|**2025-03-06**|**DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models**|Yi Shen et.al.|[2503.04472](http://arxiv.org/abs/2503.04472)|null|
|**2025-03-02**|**An Efficient and Uncertainty-aware Reinforcement Learning Framework for Quality Assurance in Extrusion Additive Manufacturing**|Xiaohan Li et.al.|[2503.00971](http://arxiv.org/abs/2503.00971)|null|
|**2025-03-03**|**On the Importance of Reward Design in Reinforcement Learning-based Dynamic Algorithm Configuration: A Case Study on OneMax with (1+( $λ$,$λ$ ))-GA**|Tai Nguyen et.al.|[2502.20265](http://arxiv.org/abs/2502.20265)|null|
|**2025-03-11**|**Pokemon Red via Reinforcement Learning**|Marco Pleines et.al.|[2502.19920](http://arxiv.org/abs/2502.19920)|**[link](https://github.com/MarcoMeter/neroRL)**|
|**2025-02-27**|**Reward Shaping to Mitigate Reward Hacking in RLHF**|Jiayi Fu et.al.|[2502.18770](http://arxiv.org/abs/2502.18770)|**[link](https://github.com/poruna-byte/par)**|
|**2025-02-21**|**On the Design of Safe Continual RL Methods for Control of Nonlinear Systems**|Austin Coursey et.al.|[2502.15922](http://arxiv.org/abs/2502.15922)|**[link](https://github.com/MACS-Research-Lab/safe-continual)**|
|**2025-02-19**|**Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning**|Hao Ma et.al.|[2502.13430](http://arxiv.org/abs/2502.13430)|null|
|**2025-02-14**|**Process Reward Models for LLM Agents: Practical Framework and Directions**|Sanjiban Choudhury et.al.|[2502.10325](http://arxiv.org/abs/2502.10325)|**[link](https://github.com/sanjibanc/agent_prm)**|
|**2025-02-18**|**A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards**|Shivansh Patel et.al.|[2502.08643](http://arxiv.org/abs/2502.08643)|null|
|**2025-02-07**|**Exploring the Generalizability of Geomagnetic Navigation: A Deep Reinforcement Learning approach with Policy Distillation**|Wenqi Bai et.al.|[2502.05069](http://arxiv.org/abs/2502.05069)|null|
|**2025-02-07**|**$TAR^2$ : Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning**|Aditya Kapoor et.al.|[2502.04864](http://arxiv.org/abs/2502.04864)|null|
|**2025-02-05**|**Demystifying Long Chain-of-Thought Reasoning in LLMs**|Edward Yeo et.al.|[2502.03373](http://arxiv.org/abs/2502.03373)|**[link](https://github.com/eddycmu/demystify-long-cot)**|
|**2025-02-04**|**CH-MARL: Constrained Hierarchical Multiagent Reinforcement Learning for Sustainable Maritime Logistics**|Saad Alqithami et.al.|[2502.02060](http://arxiv.org/abs/2502.02060)|null|
|**2025-02-04**|**Bottom-Up Reputation Promotes Cooperation with Multi-Agent Reinforcement Learning**|Tianyu Ren et.al.|[2502.01971](http://arxiv.org/abs/2502.01971)|**[link](https://github.com/itstyren/lr2)**|
|**2025-02-03**|**Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning**|Henrik Müller et.al.|[2502.01307](http://arxiv.org/abs/2502.01307)|null|
|**2025-04-28**|**CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation**|Yuanchen Yuan et.al.|[2502.00835](http://arxiv.org/abs/2502.00835)|null|
|**2025-01-31**|**An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents**|Gregory Palmer et.al.|[2501.19206](http://arxiv.org/abs/2501.19206)|null|
|**2025-01-31**|**Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach**|Wenyun Li et.al.|[2501.19128](http://arxiv.org/abs/2501.19128)|null|
|**2025-01-31**|**BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning**|Han Zhong et.al.|[2501.18858](http://arxiv.org/abs/2501.18858)|null|
|**2025-01-27**|**REINFORCE-ING Chemical Language Models in Drug Design**|Morgan Thomas et.al.|[2501.15971](http://arxiv.org/abs/2501.15971)|null|
|**2025-04-01**|**Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System**|Haikuo Du et.al.|[2501.13727](http://arxiv.org/abs/2501.13727)|**[link](https://github.com/qeneb/ss-marl)**|
|**2025-01-22**|**Deep Reinforcement Learning with Hybrid Intrinsic Reward Model**|Mingqi Yuan et.al.|[2501.12627](http://arxiv.org/abs/2501.12627)|null|
|**2025-01-19**|**Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning**|Ahmed Alagha et.al.|[2501.10938](http://arxiv.org/abs/2501.10938)|null|
|**2025-01-13**|**Online inductive learning from answer sets for efficient reinforcement learning exploration**|Celeste Veronese et.al.|[2501.07445](http://arxiv.org/abs/2501.07445)|null|
|**2025-02-20**|**AlphaPO -- Reward shape matters for LLM alignment**|Aman Gupta et.al.|[2501.03884](http://arxiv.org/abs/2501.03884)|null|

## Potential Funtions

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2002-12-19**|**DNA Torsional Solitons in Presence of localized Inhomogeneities**|Luis Morales Molina et.al.|[nlin/0112040](http://arxiv.org/abs/nlin/0112040)|null|

## Potential Funtion

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2002-12-19**|**DNA Torsional Solitons in Presence of localized Inhomogeneities**|Luis Morales Molina et.al.|[nlin/0112040](http://arxiv.org/abs/nlin/0112040)|null|

## State representation learning

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-04**|**Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation**|Siyu Wang et.al.|[2502.02327](http://arxiv.org/abs/2502.02327)|null|
|**2025-03-06**|**Learning Quadrotor Control From Visual Features Using Differentiable Simulation**|Johannes Heeg et.al.|[2410.15979](http://arxiv.org/abs/2410.15979)|**[link](https://github.com/uzh-rpg/rpg_flightning)**|
|**2025-03-11**|**Adaptive Task Allocation in Multi-Human Multi-Robot Teams under Team Heterogeneity and Dynamic Information Uncertainty**|Ziqin Yuan et.al.|[2409.13824](http://arxiv.org/abs/2409.13824)|null|
|**2024-07-18**|**On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems**|Siyu Wang et.al.|[2407.13091](http://arxiv.org/abs/2407.13091)|null|
|**2024-07-16**|**Learning telic-controllable state representations**|Nadav Amir et.al.|[2406.14476](http://arxiv.org/abs/2406.14476)|null|
|**2024-05-22**|**Maximum Manifold Capacity Representations in State Representation Learning**|Li Meng et.al.|[2405.13848](http://arxiv.org/abs/2405.13848)|null|
|**2023-12-03**|**Towards Goal-oriented Intelligent Tutoring Systems in Online Education**|Yang Deng et.al.|[2312.10053](http://arxiv.org/abs/2312.10053)|null|
|**2024-01-04**|**States as goal-directed concepts: an epistemic approach to state-representation learning**|Nadav Amir et.al.|[2312.02367](http://arxiv.org/abs/2312.02367)|null|
|**2023-10-22**|**Robust Visual Imitation Learning with Inverse Dynamics Representations**|Siyuan Li et.al.|[2310.14274](http://arxiv.org/abs/2310.14274)|null|
|**2023-10-09**|**Improving Reinforcement Learning Efficiency with Auxiliary Tasks in Non-Visual Environments: A Comparison**|Moritz Lange et.al.|[2310.04241](http://arxiv.org/abs/2310.04241)|null|
|**2023-12-15**|**On learning latent dynamics of the AUG plasma state**|A. Kit et.al.|[2308.14556](http://arxiv.org/abs/2308.14556)|**[link](https://github.com/digifusion/latent-state-modeling)**|
|**2023-11-04**|**Sparse Modular Activation for Efficient Sequence Modeling**|Liliang Ren et.al.|[2306.11197](http://arxiv.org/abs/2306.11197)|**[link](https://github.com/renll/seqboat)**|
|**2023-05-18**|**Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems**|Zhaochun Ren et.al.|[2305.11081](http://arxiv.org/abs/2305.11081)|**[link](https://github.com/hn-rs/csa)**|
|**2024-06-24**|**State Representation Learning Using an Unbalanced Atlas**|Li Meng et.al.|[2305.10267](http://arxiv.org/abs/2305.10267)|**[link](https://github.com/mengli11235/dim-ua)**|
|**2023-05-04**|**Masked Trajectory Models for Prediction, Representation, and Control**|Philipp Wu et.al.|[2305.02968](http://arxiv.org/abs/2305.02968)|**[link](https://github.com/facebookresearch/mtm)**|
|**2023-03-13**|**Unsupervised Representation Learning in Partially Observable Atari Games**|Li Meng et.al.|[2303.07437](http://arxiv.org/abs/2303.07437)|**[link](https://github.com/mengli11235/mst_dim)**|
|**2024-03-13**|**Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?**|Yi Tian et.al.|[2212.14511](http://arxiv.org/abs/2212.14511)|null|
|**2022-09-12**|**Unified State Representation Learning under Data Augmentation**|Taylor Hearn et.al.|[2209.05302](http://arxiv.org/abs/2209.05302)|**[link](https://github.com/SVJayanthi/dmcontrol-generalization-benchmark)**|
|**2022-05-04**|**State Representation Learning for Goal-Conditioned Reinforcement Learning**|Lorenzo Steccanella et.al.|[2205.01965](http://arxiv.org/abs/2205.01965)|null|
|**2022-11-14**|**Approximate Policy Iteration with Bisimulation Metrics**|Mete Kemertas et.al.|[2202.02881](http://arxiv.org/abs/2202.02881)|**[link](https://github.com/metekemertas/api-bisim)**|
|**2022-10-09**|**Mask-based Latent Reconstruction for Reinforcement Learning**|Tao Yu et.al.|[2201.12096](http://arxiv.org/abs/2201.12096)|**[link](https://github.com/microsoft/Mask-based-Latent-Reconstruction)**|
|**2022-01-18**|**Accelerating Representation Learning with View-Consistent Dynamics in Data-Efficient Reinforcement Learning**|Tao Huang et.al.|[2201.07016](http://arxiv.org/abs/2201.07016)|null|
|**2022-07-26**|**DiffSRL: Learning Dynamical State Representation for Deformable Object Manipulation with Differentiable Simulator**|Sirui Chen et.al.|[2110.12352](http://arxiv.org/abs/2110.12352)|**[link](https://github.com/ericcsr/diffsrl)**|
|**2022-06-19**|**Action-Sufficient State Representation Learning for Control with Structural Constraints**|Biwei Huang et.al.|[2110.05721](http://arxiv.org/abs/2110.05721)|null|
|**2021-10-02**|**Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning**|Elie Aljalbout et.al.|[2110.00784](http://arxiv.org/abs/2110.00784)|null|
|**2022-02-15**|**Exploratory State Representation Learning**|Astrid Merckling et.al.|[2109.13596](http://arxiv.org/abs/2109.13596)|**[link](https://github.com/astrid-merckling/SRL4RL)**|
|**2021-09-28**|**Making Curiosity Explicit in Vision-based RL**|Elie Aljalbout et.al.|[2109.13588](http://arxiv.org/abs/2109.13588)|null|
|**2023-12-09**|**POAR: Efficient Policy Optimization via Online Abstract State Representation Learning**|Zhaorun Chen et.al.|[2109.08642](http://arxiv.org/abs/2109.08642)|**[link](https://github.com/billchan226/poar-srl-4-robot)**|
|**2021-09-14**|**Comparing Reconstruction- and Contrastive-based Models for Visual Task Planning**|Constantinos Chamzas et.al.|[2109.06737](http://arxiv.org/abs/2109.06737)|null|
|**2021-07-04**|**Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces**|Nicolò Botteghi et.al.|[2107.01667](http://arxiv.org/abs/2107.01667)|null|
|**2021-06-28**|**Improving Prediction of Low-Prior Clinical Events with Simultaneous General Patient-State Representation Learning**|Matthew Barren et.al.|[2106.14838](http://arxiv.org/abs/2106.14838)|null|
|**2021-06-09**|**Pretrained Encoders are All You Need**|Mina Khan et.al.|[2106.05139](http://arxiv.org/abs/2106.05139)|**[link](https://github.com/PAL-ML/PEARL_v1)**|
|**2021-03-07**|**Multimodal VAE Active Inference Controller**|Cristian Meo et.al.|[2103.04412](http://arxiv.org/abs/2103.04412)|**[link](https://github.com/Cmeo97/MAIF)**|
|**2021-03-07**|**Learning a State Representation and Navigation in Cluttered and Dynamic Environments**|David Hoeller et.al.|[2103.04351](http://arxiv.org/abs/2103.04351)|null|
|**2021-11-05**|**Learning State Representations from Random Deep Action-conditional Predictions**|Zeyu Zheng et.al.|[2102.04897](http://arxiv.org/abs/2102.04897)|**[link](https://github.com/Hwhitetooth/random_gvfs)**|
|**2021-01-07**|**Low Dimensional State Representation Learning with Reward-shaped Priors**|Nicolò Botteghi et.al.|[2007.16044](http://arxiv.org/abs/2007.16044)|null|
|**2020-05-13**|**Progressive growing of self-organized hierarchical representations for exploration**|Mayalen Etcheverry et.al.|[2005.06369](http://arxiv.org/abs/2005.06369)|null|
|**2021-02-16**|**Graph-based State Representation for Deep Reinforcement Learning**|Vikram Waradpande et.al.|[2004.13965](http://arxiv.org/abs/2004.13965)|null|
|**2020-02-27**|**Acceleration of Actor-Critic Deep Reinforcement Learning for Visual Grasping in Clutter by State Representation Learning Based on Disentanglement of a Raw Input Image**|Taewon Kim et.al.|[2002.11903](http://arxiv.org/abs/2002.11903)|null|
|**2021-06-04**|**Domain-Adversarial and Conditional State Space Model for Imitation Learning**|Ryo Okumura et.al.|[2001.11628](http://arxiv.org/abs/2001.11628)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

